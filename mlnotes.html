<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Machine Learning Notes</title>
    <style>
        li {
          color: blue;
        }
        li > span {
            color: black;
        }
        img {
            width: 300px;
            height: auto;
        }
        .row {
            display: flex;
            flex-wrap: wrap;
            flex-direction: row;
            grid-column-gap: 20px;
        }

    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <h1 style = "background: linear-gradient(to right, blue 35%, white 35%, white 65%, red 30%); border-radius: 4px; text-align: center;">Machine Learning Notes</h1>
    <h2 style = "text-decoration: underline; "> Lecture 12-13: PAC Learning and VC Theory</h2>
    <h3>Generalization error and empirical error</h3>
    <ul>
        <li>Generalization error: \(  R(\hat{c}) :=  \mathbf{P}(\hat{c}(X)\neq c(X)) \)  <span>not computable by the learner</span> </li>
        <li>Empirical error: \( \hat{R}_n(\hat{c}) := \frac{1}{n} \sum_{i \leq n}{1_{\hat{c}(x_i) \neq c(x_i)}}  \) </li>
    </ul>
    <p> The empirical risk is an unbiased estimator of the expected risk since \( \mathbb{E}_{X = (X_1, X_2, .., X_n)} [\hat{R}_n (\hat{c}(X))  ] = \frac{1}{n} \sum_{i \leq n}{  \mathbb{E} ( 1_{\hat{c}(x_i) \neq c(x_i)}) }  =  \frac{1}{n} \sum_{i \leq n}{   \mathbf{P}(\hat{c}(x_i) \neq c(x_i)})  =   \mathbf{P}(\hat{c}(x_i) \neq c(x_i))  \)</p>  
Some terminology:
    <ul>
        <li>Concept: <span>A concept is a subset of \( X \) that maps to 1, the remaining values of \( X \) mapping to 0</span></li>
        <li>Concept class: <span>a set of concepts we wish to learn</span> </li>
        <li>Hypothesis class: <span>Another set of concepts we use to learn a target concept from the concept class</span> </li>
    </ul>

    The target concept correctly maps all values of \(X \) (in the deterministic case).

    <h3>The PAC Learning Model</h3>
    A learning algorithm \(A \) can learn a concept \( c \) from \(C \) if, given as input a sufficiently large sample, it outputs a hypothesis that generalizes well with high probability
    <br>
    <h4>Definition</h4> 
    A learning algorithm \(A \) can learn a concept \(c \) if there is a polynomial function \( poly(\cdot, \cdot, \cdot )\) such that 
    <ol>
        <li>For any distribution \( D\) on \( X\) and </li>
        <li> for any \( 0 < \epsilon < \frac{1}{2}, 0 < \delta < \frac{1}{2}\)</li>
    </ol>
    if \( A\) receives as input a sample \( Z\) of size \( n \geq poly(\frac{1}{\epsilon}, \frac{1}{\delta},\text{size}(c)) \) then \(A \) outputs \(\hat{c} \) such that 
    \[ \mathbf{P}_{Z \sim D^n} (R(\hat{c}) \leq \epsilon ) \geq 1 - \delta \]

    A concept class \(C \) is PAC-learnable from a hypothesis class \( H\) if there is an alogrithm that can learn any concept in \(C \) using a hypothesis from \(H \).
    <h3>Example: Axis-aligned rectangles are PAC-learnable</h3>
    <div class="row">
        <img src="img/aarect.png" alt="Picture of axis-aligned rectangle with positive points in the interior, negative in the exterior">
        
        <img src="img/lernrect.png" alt="Picture of the hypothesis rectangle which tightly fits positive points"> 
    </div>
    Where the algorithm \( A\) outputs the smallest fitting rectangle \(\hat{R} \).
    To prove learnability we show that \(\mathbf{P} (R(\hat{R}) \leq \epsilon ) \geq \mathbf{P}(\hat{R}IG) \geq 1-4\exp(-\frac{n\epsilon}{4}) \geq 1-\delta \) 
    <br> where the last inequality implies that \( n\geq \displaystyle\frac{4}{\epsilon} \ln\frac{1}{\delta} \geq  \displaystyle\frac{4}{\epsilon}\frac{1}{\delta} \)
    <br> \(\hat{R}IG \) is the event that a rectangle intersects all four strips (upper, lower, left, right strips) where each strip has probability mass distribution \(\displaystyle\frac{\epsilon}{4} \). The following rectangle, for example, is not RIG.
    <br> <img src="img/notrig.png" alt="not RIG rectangle" style ="width: 400px;">  <br>
     
    To show that   \(\mathbf{P} (R(\hat{R}) \leq \epsilon ) \geq \mathbf{P}(\hat{R}IG)  \), or that the event RIG implies the former, we prove the contrapositive: <br> <br>
    \( \mathbf{P}_{S \sim  D^m} (R(R_s) > \epsilon) \leq \mathbf{P}_{S \sim D^m} (\cup_{i = 1}^{4} \{ R_s \cap r_i = \emptyset \} )  \leq \sum_{i=1}^{4} \mathbf{P}(\{ R_s \cap r_i = \emptyset \}) \text{ (by the union bound)} \leq 4(1 - \epsilon/4)^{m} \leq 4\exp{(-m\epsilon/4)}  \text{ (since } 1-x \leq e^{-x} ) \)
   <br>
   which means that if the error area is to be larger that \( \epsilon \) then the learned rectangle cannot intersect all strips (otherwise the error area of the rectangle will be less than \( \epsilon \)). <br>
   \( 4\exp{(-m\epsilon/4)}  \leq \delta \Leftrightarrow  m \geq \frac{4}{\epsilon}\log{\frac{4}{\delta}} \) since log is subpolynomial then we can change the bound by replacing \( \log{\frac{4}{\delta}} \) to \( \frac{4}{\delta} \)  
   <h3>Universal concept class</h3>
    The universal concept class, i.e. the concept class formed by all subsets of binary vectors of fixed length, is not PAC-learnable but the proof is hard. 

    <h3>Consistent hypothesis for finite hypothesis classes</h3>
    Let \( C\) be a finite concept class and assume that \( \mathcal{H} = C \). Let \( \mathcal{A} \) be an algorithm that returns a consistent hypothesis \( \hat{c} \) (i.e. \( \forall n < \infty: R_n(\hat{c}) =0 \)) for any target concept \( c \in \mathcal{C} \)
    and any i.i.d sample \(\mathcal{Z} \). For any \(\epsilon, \delta > 0 \) if \[ n \geq \frac{1}{\epsilon} (\log|\mathcal{H}| + \log{\frac{1}{\delta}} ) \] then 
    \( \mathbf{P}(R(\hat{c}) \leq \epsilon ) \geq 1-\delta  \). 
    <br> <br>
    Proof: <br>
    \( \mathbf{P}(R(\hat{c}) > \epsilon)  \leq \mathbf{P}\{ \max_{c \in \mathcal{C}: R_n(c) = 0} R(c) > \epsilon \}  = \mathbb{E}\{ \displaystyle I_{\max_{c \in \mathcal{C}: R_n(c) = 0 } R(c) > \epsilon } \} = \mathbb{E}\{ \max_{c \in \mathcal{C}} I_{R(c)>\epsilon}I_{R_n(c) =0} \} \leq \mathbb{E}\{ \sum_{c \in \mathcal{C}} I_{R(c)>\epsilon}I_{R_n(c) =0} \} \leq \mathbb{E}\{ \sum_{c \in \mathcal{C}: R(c) > \epsilon} I_{R_n(c) =0} \} \leq |\mathcal{C}|(1-\epsilon)^n \leq  |\mathcal{C}|\exp(-\epsilon n) \leq \delta \)
    <h3>Useful Inequalities</h3>
    <ol>
        <li> \( R(\hat{c}^*_{n}) - \inf_{c \in \mathcal{C}}R(c) < 2\sup_{c \in \mathcal{C}}|\hat{R}_n(c) - R(c)| \) where \( \hat{c}^*_n = \arg \min_{c \in \mathcal{C}} \hat{R}_n(c)  \)  </li>
        <li> \( \forall \epsilon > 0, \ \mathbf{P}(|\hat{R}_n(c) - R(c)| > \epsilon) \leq 2|\mathcal{C}|\exp{(-2n\epsilon^2)} \)</li>
        <li> \( \mathbf{P}(R(\hat{c}^*_n) - \inf_{c \in \mathcal{C}}R(c) > \epsilon ) \leq  2|\mathcal{C}|\exp{(-\frac{n\epsilon^2}{2})} \) <span>can be demonstrated from the two above</span> </li>
    </ol>
    The proof for (1) follows: <br>
    Let \(c^* \in \arg \min_{c \in \mathcal{C}} R(c) \text{ and } \hat{c} \in  \arg \min_{c \in \mathcal{C}} \hat{R}_n(c)  \) then \( R(\hat{c})  - \inf_{c \in \mathcal{C}} R(c) = ( R(\hat{c}) - \hat{R}_n(\hat{c}) ) + (\hat{R}_n(\hat{c}) - R(c^*))  \leq ( R(\hat{c}) - \hat{R}_n(\hat{c}) ) + (\hat{R}_n(c^*) - R(c^*))   \) <br>
    note that both parenthesized terms are each less than \( \sup_{c \in \mathcal{C}} |R(c) - \hat{R}_n(c)| \), which completes the proof. 
    <br> <br>
    The proof of (2) requires<b> Hoeffding's inequality </b> (Foundations of Machine Learning by Mohri et al): <br>
    Let \(X_1, ..., X_m \) be independent random variables with \(X_i \) taking values in [\(a_i, b_i\)] for all \( i\in [m] \). Then for any \(\epsilon > 0 \), the following inequalities hold for \(S_m = \sum_{i=1}^{m}X_i \),
    \[ \mathbf{P}(S_m - \mathbb{E}[S_m] \geq \epsilon ) \leq e^{-2\epsilon^2/\sum_{i=1}^{m}(b_i - a_i)^2}  \]
    \[ \mathbf{P}(S_m - \mathbb{E}[S_m] \leq -\epsilon ) \leq e^{-2\epsilon^2/\sum_{i=1}^{m}(b_i - a_i)^2} \] 
    Now using the fact \(n\hat{R}_n(\hat{c}) = \sum_{i = 1}^{n} 1_{c(x_i) \neq \hat{c}(x_i)} =S_n  \) and \(X_i = 1_{c(x_i) \neq \hat{c}(x_i)} \) as well as the union bound completes the proof. 

    <br> <br>
    <b>Problem 3 (Learning concentric circles)</b> from the HW:
    <br>
    Consider the instance space \(\mathcal{X} = \mathbb{R}^2 \) and the hypothesis class \( \mathcal{C} = \{\mathcal{C}_r : r \in \mathbb{R} \}  \) consisting of all indicator functions of circles with center at (0, 0): 
    \[ C_r(\mathbf{x}) = \begin{cases} 
    0 & ||\mathbf{x}||_2 > r \\
    1 & ||\mathbf{x}||_2 \leq r 
 \end{cases} \]
 Fix some \(r^* \in \mathbb{R} \) and consider the target classifier \(c^* = C_{r^*} \). Let \(X_1, X_2, ... \) be i.i.d random variables taking values in \(\mathbb{R}^2 \) and let \(Y_i := c^*(X_i) \).
 Moreover, for \( n \in \mathbb{N} \), let \(\hat{c}_n := C_{r_\min^n}  \)  be the tightest fitting consistent circle with 
 \[ r_\min^n := \underset{i \leq n, Y_i = 1}{\max} ||X_i||_2 \]
 1. Let \( \epsilon > 0 \), and assume there exists a unique \( r_{\epsilon}^+ \in \mathbb{R}\) s.t. \(\mathbf{P}(r_{\epsilon}^+ < ||X_i||_2 \leq r^*) = \epsilon \). Show that \(\mathbf{P}(r_{\min}^n \leq r_{\epsilon}^+) = (1 - \epsilon)^n \)
 <br> Ans:  \(\mathbf{P}(r_{\min}^n \leq r_{\epsilon}^+) = \mathbf{P}(\underset{ i \leq n, Y_i = 1}{\bigwedge} ||X_i||_2 \leq r_{\epsilon}^+)  = \displaystyle\prod_{i=1}^{n} \mathbf{P} (||X_i||_2 \leq r_{\epsilon}^+) \) since by definition \( ||X_i||_2 \leq r^* \) then the probability mass of \( ||X_i||_2 \) is distributed in \([0, r^*] \) thus 
 \( \displaystyle\prod_{i=1}^{n} \mathbf{P}(||X_i||_2 \leq r_{\epsilon}^+) =  \displaystyle\prod_{i=1}^{n} (1 - \mathbf{P}(r_{\epsilon}^+ < ||X_i||_2 \leq r^*)) = (1-\epsilon)^n   \)
 <br>
 2. Show that if \( n \geq \frac{1}{\epsilon}\log{\frac{1}{\delta}} \), then \(\mathbf{P}(r_{\min}^n \leq r_{\epsilon}^+) \leq \delta, \text{ for } \delta > 0 \). <br>
 Ans: Alternatively show the contrapositive: \( \mathbf{P}(r_{\min}^n \leq r_{\epsilon}^+) > \delta \Rightarrow n <  \frac{1}{\epsilon}\log{\frac{1}{\delta}}  \). We have \( (1-\epsilon)^n > \delta \Rightarrow e^{-n\epsilon} > \delta \Rightarrow n <  \frac{1}{\epsilon}\log{\frac{1}{\delta}}  \) <br>
 3. Show that \( \mathcal{C} \) is efficiently PAC-learnable. <b> <i> Hint: show that \( \mathbf{P}(R(\hat{c}_n) \geq \epsilon) = \mathbf{P}(r_{\min}^n \leq r_{\epsilon}^+) \).
</i></b> <br>
 Ans: Observe that \[R(\hat{c}_n) = \mathbf{P}(\hat{c}_n(X_i) \neq c^{*}(X_i) ) = \mathbf{P}(r_{\min}^n < ||X_i||_2 \leq r*)  \]
 If \( r_{\min}^n \leq r_{\epsilon}^+ \), then by construction, 
    \[ \mathbf{P}(r_{\min}^n < ||X_i||_2 \leq r^*) \geq \mathbf{P}(r_{\epsilon}^+ < ||X_i||_2 \leq r^*) \]
    which means \( R(\hat{c}_n) \geq \epsilon\). Observe that if \(r_{\min}^n > r_{\epsilon}^+ \), then \(  R(\hat{c}_n) < \epsilon \). Thus, \( \mathbf{P}(R(\hat{c}_n) \geq \epsilon) = \mathbf{P}(r_{\min}^n \leq r_{\epsilon}) \leq \delta \) for \( n \geq \frac{1}{\epsilon}\log\frac{1}{\delta} \). In other words, for \( n \geq \frac{1}{\epsilon}\log\frac{1}{\delta}\), 
    \[ \mathbf{P}(R(\hat{c}_n) \leq \epsilon) \geq 1-\delta  \] <br>
    Thus, \(\mathcal{C} \) is PAC learnable. Moreover, \( \mathcal{C} \) is efficiently PAC learnable since computing \(r_{\min}^n\) is \(O(n)\) and we require at least \(\frac{1}{\epsilon}\log\frac{1}{\delta}\) instances, i.e. the proposed algorithm runs in time polynomial in \(\frac{1}{\epsilon} \text{ and } \frac{1}{\delta}\).
    <br>
    <h2 style = "text-decoration: underline; "> Lecture 11: EM and Non-parametric Bayesian Methods </h2>
    <h3>
        <i>k</i> -Means Algorithm
    </h3>
    Given <i>d</i>-dimensional sample vectors \(\mathcal{X} = \{\mathbf{x_1},..., \mathbf{x_n}\} \) find an assignment function \[ \begin{gather*} c: \mathbb{R}^d \rightarrow \{1, ..., k\} \\ \mathbf{x} \mapsto c(\mathbf{x}) \end{gather*} \]
    such that \( \mathcal{R}^{k}(c, \mathcal{Y}) = \sum_{\mathbf{x} \in \mathcal{x}} ||\mathbf{x}-\mu_{c(\mathbf{x})}||^2  \) is minimized where \(\mathcal{Y} \) is the cluster protoypes (centroids) and \(\mu_c \in \mathcal{Y}, c\in \{1, ..., k\}\).
    <br> <br>
    <div style = "background:rgba(37, 250, 0, 0.3); font-family: sans-serif; width: 500px; padding-bottom: 10px; padding-right: 10px;">
    <div style="font-weight: 600; text-align: center; width: 100%; padding-top: 10px; display: inline-block;"><i>k</i>-Means Algorithm </div> <br>
    <ol>
        <li> Initialize prototypes to arbitrary values </li>
        <li>Assign each \(\mathbf{x}\) to its closest prototype (i.e. define function \(c(\mathbf{x})\))</li>
        <li>
            Estimate new prototypes by taking averages of points assigned to the same prototype from the previous step. 
        </li>
        <li>
            Iterate until changes of \( c(\mathbf{x}), \mathcal{Y}\) vanish 
        </li>
        <li>
            return the prototypes and assignment function
        </li>
    </ol>
      
    </div>
    <br>
    The problem with <i>k-</i>means is that it gives hard assignments without probabilities. To solve this problem where we have <i>k</i> sources producing samples one may use probabilistic mixture models. 
    <h3>Mixture Models</h3>
    Often, various sources are considered as potential causes for an observed sample. This model class is known as a mixture model with the density for a feature vector \(\mathbf{x} \sim p(\mathbf{x}|\theta) = \)
    \[ p(\mathbf{x} | \pi_1,..., \pi_k, \mathbf{\theta_1}, ..., \mathbf{\theta_k} ) = \sum_{c\leq k} \pi_c p(\mathbf{x}|\mathbf{\theta_c}) \]
    where the mixture weight \( \pi_c\) denotes the prior probability that a sample is generated by the mixture component \(c\) with parameters \(\mathbf{\theta_c}\). 
   <br> \( \mathbf{\theta_c}\) depends on what kind of model you have chosen for your mixture component, so if \(p\) is essentially a normal distribution then \(\mathbf{\theta_c}\) specifies variance and mean. 
   <h3>Gaussian Mixtures</h3> 
   <ul>
       <li>
           Gaussian mixtures: <span>parameters \( \mathbf{\theta} = (\mathbf{\mu}, \sum), \ \ p(\mathbf{x}| \mathbf{\mu, \sum} ) = \frac{1}{\sqrt{2\pi}^d}\frac{1}{\sqrt{|\mathbf{\sum}|}}\exp{(-\frac{1}{2}(\mathbf{x-\mu})^T\mathbf{\sum}^{-1} (\mathbf{x - \mu}))}  \)</span> 
       </li>
       <li>
           Estimate <span>\( \hat{\theta} \) such that it maximizes the likelihood of sample feature vectors \(\mathcal{X} = \mathbf{\{x_1, ..., x_n\}}, \ \ p(\mathcal{X}| \pi_1, ..., \pi_k,\theta_1, ..., \theta_k) = \displaystyle\prod_{\mathbf{x} \in \mathcal{X}}\displaystyle\sum_{c \leq k}\pi_cp(\mathbf{x}|\mathbf{\theta_c}) \)</span>
       </li>
       <li>
           Log-likelihood  <span> is often computationally preferable: \( L(\mathcal{X}| \pi_1, ..., \pi_k, \mathbf{\theta_1, ..., \theta_k}) = \displaystyle\sum_{\mathbf{x} \in \mathcal{X}}\log\displaystyle\sum_{c \leq k}\pi_cp(\mathbf{x}|\mathbf{\theta_c})  \)</span> 
       </li>
   </ul>
   However, direct maximization is intractable due to sum within logarithm; thus, we introduce latent (hidden) variables, \(z\), which correspond to (expected) data assignments.
   <br>
   Thus the "latent variable form":
   <ul>
    <li> 
        \(p(z = c) = \pi_c \ \ \) <span>  probability of \(\mathbf{x}_i\) being assigned to mixture mode c</span>
    </li>
    <li>
      \(  p(x|z = c) = \mathcal{N}(x ; \mu_c, \sigma_c) \)
    </li>
   </ul>
   <div style = "background:rgba(37, 250, 0, 0.3); font-family: sans-serif; width: 500px; padding-bottom: 10px; padding-right: 10px;">
    <div style="font-weight: 600; text-align: center; width: 100%; padding-top: 10px; display: inline-block;">Expectation Maximization Algorithm </div> <br>
    <ol>
        <li> E-step
            <ol>
                <li>Start with clusters: Mean \(\mu_c\), covariance \(\Sigma_c\), and size \(\pi_c\)</li>
                <li>For each datum \(\mathbf{x_i}\), compute \(\gamma_{\mathbf{x}c}\) the probability that it belongs to \(c\). For the case of GMM,  \(\gamma_{\mathbf{x_i}c} = \frac{\pi_c \mathcal{N}(\mathbf{x}_i; \  \mu_c, \sum_c) }{\sum_{c'}\pi_{c'} \mathcal{N}(\mathbf{x}_i; \  \mu_{c'}, \sum_{c'})  } \)</li>
            </ol>
        </li>
        <li> M-step 
            <ol>
                <li> <span> Update mixture weights </span> \( \pi_c = \frac{1}{|\mathcal{X}|}\displaystyle\sum_{\mathbf{x}\in \mathcal{X}} \gamma_{\mathbf{x}c} \)</li>
                <li>
                    <span>Update means </span> \( \mathbf{\mu}_c = \frac{\sum_{\mathbf{x} \in \mathcal{X}} \mathbf{x}\gamma_{\mathbf{x}c}  }{\sum_{\mathbf{x} \in \mathcal{X}} \gamma_{\mathbf{x}c} } \)
                </li>
                <li>
                    <span>Update variances:</span> \(\Sigma_c = \sigma_c^2\mathbb{I}, \  \sigma_c^2 = \frac{\sum_{\mathbf{x} \in \mathcal{X}} \gamma_{\mathbf{x}c}(\mathbf{x - \mu_c})^2 }{\sum_{\mathbf{x} \in \mathcal{X} } \gamma_{\mathbf{x}c} }\)
                </li>
            </ol>
        </li>
        <li>Repeat until changes small enough</li>
        
    </ol>
      
    </div>
    <h3>Non-paremetric Bayesian</h3>
    <b>Recall Beta Distribution</b> <br>
    \(Beta(x|a, b) = \frac{1}{B(a,b)} \cdot x^{a-1}(1-x)^{b-1} \), where \(x \in [0, 1];\ \ a, b>0\) a probability distribution over \(x\). <br>
    \(B(a, b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}\) is the beta function where \(\Gamma(a) = \int_{0}^{\infty} e^{-x}x^{a-1}dx\) 
    <br>
    The multivariate generalization of the beta distribution is the Dirichlet distribution: \[\text{Dir}(\mathbf{x}|\mathbf{\alpha}) = \frac{1}{B(\alpha)} \cdot \prod_{k=1}^n x_k^{\alpha_k - 1}\]
    where \(B(\mathbf{\alpha}) = \displaystyle\frac{\prod_{k=1}^n \Gamma(\alpha_k)}{\Gamma(\sum_{k=1}^n \alpha_k)}\) and \(\mathbf{x} \text{ and } \mathbf{\alpha}\) are vectors of length <i>n</i> with only positive components and \(x_i \in  [0, 1]\). 
    <br><b>Latent clusters</b> <br>
    For a finite number of drawings <i>N</i>, we do not have to realize <b> all</b> K clusters. All K clusters will be realized with probability 1, but only when \(N \rightarrow \infty \).
    <br> How to handle need of new clusters? Select large K. But then the problem is solved only partially since any specific K might have problems and our belief in K could change as we observe more data points. Solution is thus to use K = \(\infty \) which is a non-parametric bayesian method. In reality this would mean letting the data "speak" how many clusters it needs. 
</body>
</html>
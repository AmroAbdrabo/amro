<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href = "syssec.css">
    <title>Program Analysis</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<style>
  code {
  font-size: 1rem;
  background: rgba(0,0,0,0);
  border-radius: 0px;
  padding: 0px 0px;
}
pre{
  font-size: 1rem;
  border-style: solid; 
  border-radius: 0.5cm; 
  width: min(90vw, 690px);
  border-color: blueviolet;
  overflow: auto;
}
</style>
<body>


    <div class="topnav" style = "background-color: rgb(91, 16, 112);">
        <a class="active" href="index.html" style = "background-color: rgb(146, 135, 211);">Home</a>
      </div>
      
     
   
      <script async="async" defer="defer" src="https://buttons.github.io/buttons.js"></script>
      <div class="container">
        <div class="meta"> 
          <div class="image" style="background: url(img/ether.jpg); background-size:cover; width: 95%; background-position: center;"></div>
          <div class="info"> 
            <h1>Program Analysis for Security and Reliability </h1>
            <p class="subtitle">Course by Prof. Dr. Vechev</p>
            <div class="author">
              <div class="authorImage"></div>
              <div class="authorInfo">
                <div class="authorName"><a href="https://github.com/AmroAbdrabo">Amro Abdrabo</a></div>
                <div class="authorSub">April 2022 <span class="median-divider"> </span> </div>
              </div>
            </div>
          </div>
        </div>
  <main class="article">

    <h1> Data flow analysis in Datalog</h1>
    <p>
      Suppose we are given the following code (line numbering omitted):
    </p>
    <div style="background: rgb(230, 230, 230); width: fit-content; padding-right: 35px; margin-bottom: 40px;">
      <pre style="border: 0ch; width: fit-content;">
     <code  class="language-pascal">
      x = source()
      y = x + z
      if (y > 0){
        y = x + 4
      }
      else {
        z = 2
      }
      sink(y)
     </code>
     </pre>
  </div>
    
     
  

  <p>
    The semantics are
    <ul>
      <li> Reading untrusted input<code> x = source()</code></li>
      <li>
        Assignments: <code> y = x + z </code> 
      </li>
      <li>
        Branching
      </li>
      <li>
        Output sink: <code>sink(y)</code> outputs the value of variable <code>y</code>  to output 
      </li>
    </ul>
  </p>

  <p>
    To write a datalog program that captures these semantics (encodes these programs), then we have:
  </p>

  <p>
    
    

    <pre style="border-style: solid; border-radius: 0.5cm; border-color: blueviolet; overflow: scroll;">

  // <code>L: X = source()</code> 
  <code>.decl source(L: number, X: symbol)</code>  

  // <code>L: sink(X)</code> 
  <code>.decl source(L: number, X: symbol)</code> 

  // <code>L: X = Y+Z</code> 
  <code>.decl assign(L: number, X: symbol, Y: symbol, Z: symbol)</code> 

  // <code>L1: ...</code> 
  // <code>L2: ...</code> 
  <code>.decl next(L1: number, L2: number)</code> 

  // <code>L1: if(X) { </code> 
  // <code>L2:  ... </code> 
  // <code> } </code> 
  // <code>L3:  ... </code> 
  // <code> } else { </code> 
  <code>.decl if(L1: number, L2: number, L3: number)</code> 

  // <code>if   { </code> 
  // <code>    ... </code> 
  // <code> L2: } </code> 
  // <code>else { </code> 
  // <code>     ... </code> 
  // <code> L3: } </code> 
  // <code> L1: ... </code> 
  <code>.decl join(L1: number, L2: number, L3: number)</code> 
  </pre>
  
  </p>

  <p>
    A value is unsanitized if its value is derived from user input (ignoring implicit control-flow dependencies). 
    Here we see a datalog program that calculates at every line all tainted values <b> before and after </b>
execution of that line and calculates if a sink may accept an unsanitised user input:
  </p>

<p>
  
  <pre style="border-style: solid; border-radius: 0.5cm; border-color: blueviolet;overflow: scroll;">

  // derived predicates
  .decl taintedBefore(L:number, X:symbol)
  .decl taintedBefore(L:number, X:symbol)

  // The sink argument at label L is tainted
  .decl unsanitized(L: number)
  .output unsanitized

  // After execution of source at line L (unsanitised user input) 
  // the value X after line L is unsanitised
  taintedAfter(L, X) :- source(L, X).

  // Line L is unsanitized if X is the outputted at line L 
  // and X was tainted before line L
  unsanitized(L) :- sink(L, X), taintedBefore(L, X).

  // X is tainted after line L if it was tainted 
  // before line L and it was not assigned something else
  taintedAfter(L, X) :- taintedBefore(L, X), !assign(L, X, _, _).
  
  // X is tainted after L if it was assigned at a tainted value on L
  taintedAfter(L, X) :- assign(L, X, Y, _), taintedBefore(L, Y).
  taintedAfter(L, X) :- assign(L, X, _, Z), taintedBefore(L, Z).

  // X is tainted before a line if it was tainted 
  // after an earlier line (all possible cases)
  taintedBefore(L2, X) :- next(L1, L2), taintedAfter(L1, X).
  taintedBefore(L2, X) :- if(L1, L2, _), taintedAfter(L1, X).
  taintedBefore(L3, X) :- if(L1, _, L3), taintedAfter(L1, X).
  taintedBefore(L1, X) :- join(L1, L2, _), taintedAfter(L2, X).
  taintedBefore(L1, X) :- join(L1, _, L3), taintedAfter(L3, X).
  </pre>

</p>
  <p>
    Now assume we have a sanitize function <code>sanitize(x)</code> which can be used to "sanitize" variables. To include this functionality in Datalog, note that
    we already had a similar functionality when we specified x to be tainted-after only if it is NOT assigned to something else. 
  </p>

  <pre style="border-radius: 0.2cm;">

  taintedAfter(L, X) :- taintedBefore(L, X), !assign(L, X, _, _), !sanitize(X).
  </pre>
  <p>
    The full project for a taint-analyzer on Ethereum smart contracts can be found at this 
    <a href="https://github.com/AmroAbdrabo/ethereum-sc-taint-analyzer"> GitHub link</a>. As for now, 
    let us focus on ways to attack models so that they (perhaps correctly) output "garbage dump" when they are fed an image of a WOKO student housing complex.
  
  </p>
  
     

  <h1> Black-box Attacks on Machine Learning Models</h1>
  
 <p>
  The defining question of this topic is whether ML can be deployed to the real world. Let us see some of the attacks against ML models.
 </p>
 <p>
  Imagine you are training a model to detect pneumothorax in lungs using a chest X-ray. The doctor attaches a drain to the chest and the patient 
  has an X-ray of his chest taken. Now, when the ML model is trained on this image, it will notice the presence of a drain. Of course, the probability that a 
  patient has pneumothorax given he has a drain is much higer than the probability of them having the disease with no drain attached. And that is exactly what the model would do.
  Given an X-ray showing a drain, its prediction will be biased towards showing presence of the disease, unlike the doctor who would make his decision independent of the drain (to some extent).  
 </p>
<p>
  There are certain threats to ML security:
</p>
<p>
  <ul>
    <li>Model inversion: inverting the model to obtain the dataset</li>
    <li>
      Membership inference: given the model, knowing whether a sample was used in the training dataset
    </li>
    <li>
      Adversarial attack: given a correctly classified image, can the image be slightly changed so that it is no longer correctly classified
    </li>
    <li>
      Data poisoning: adversary injects into the training set a few correctly classified images with a small and specific pixel pattern that biases the model towards that pixel pattern
    </li>
    <li>
      Backdoor ML: involves the use of data poisoning but with the attack component being incorrect label produced during inference
    </li>
  </ul>
</p>
<p>
  All images are affected by adversarial attacks, and even the best model is affected by adversarial attacks. 
</p>
<p>
  The problem is that state of the art models are accurate but very brittle.
</p>

<h2>
  Adversarial robustness
</h2>
<p>
  In order to achieve adversarial robustness, one must specify two components: 
  <ul>
    <li>
      Specification: what are the allowed changes to the image (e.g. rotation, sheering, lighting, scaling, masked parts, etc.)  
    </li>
    <li>
      Optimization: given the image, embedded into the input space, and the allowed modifications (defining a hyperplane of samples around the original input), can we obtain an incorrect label for a sample from this hyperplane. 
    </li>
  </ul>

  <img src="img/spec_opt.PNG" alt="" style = "width: 90%; height: auto; margin-left: auto; margin-right: auto; display: block;">  
</p>

<p>
  Recall the task of model training is to find \[ \text{argmin}_{\theta} E_{(\textbf{x, y}) \sim  D} \ loss(\theta, \textbf{x}, \textbf{y})    \]
</p>
<p>
  However, for adversarial attack the task is to find,  \[  \text{argmax}_{|| \delta|| < \epsilon }  \ loss(\theta, \textbf{x}+\delta, \textbf{y})  \]
</p>
<p>
  with the update being,
</p>
<p>
  \[\delta \leftarrow \delta - \nabla loss(\theta, \textbf{x}+\delta, \textbf{y}) \]
</p>
<h2>
  Attack taxonomy
</h2>
<p>
  <img src="img/att_tax.png" alt="" style = "width: 90%; height: auto; margin-left: auto; margin-right: auto; display: block;">  
</p>
<p>
  Note that as we go right-down the attack difficulty increases. 
</p>
   
  </main>
</div>
</body>
</html>
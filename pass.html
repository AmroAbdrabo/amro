<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href = "syssec.css">
    <title>Program Analysis</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<style>
  code {
  font-size: 1rem;
  background: rgba(0,0,0,0);
  border-radius: 0px;
  padding: 0px 0px;
}
pre{
  font-size: 1rem;
  border-style: solid; 
  border-radius: 0.5cm; 
  width: min(90vw, 690px);
  border-color: blueviolet;
  overflow: auto;
}
.algo {
  background: rgba(12, 12, 183, 0.2); padding-right: 35px; margin-bottom: 40px; display: block; margin-left: auto; margin-right: auto; border-radius: 0.5cm;
}
.algo_pre{
  border: 0ch; width: fit-content; display: block; margin-right: auto; margin-left: auto;
}
</style>
<body>


    <div class="topnav" style = "background-color: rgb(91, 16, 112);">
        <a class="active" href="index.html" style = "background-color: rgb(146, 135, 211);">Home</a>
      </div>
      
     
   
      <script async="async" defer="defer" src="https://buttons.github.io/buttons.js"></script>
      <div class="container">
        <div class="meta"> 
          <div class="image" style="background: url(img/ether.jpg); background-size:cover; width: 95%; background-position: center;"></div>
          <div class="info"> 
            <h1>Program Analysis for Security and Reliability </h1>
            <p class="subtitle">Course by Prof. Dr. Vechev</p>
            <div class="author">
              <div class="authorImage"></div>
              <div class="authorInfo">
                <div class="authorName"><a href="https://github.com/AmroAbdrabo">Amro Abdrabo</a></div>
                <div class="authorSub">April 2022 <span class="median-divider"> </span> </div>
              </div>
            </div>
          </div>
        </div>
  <main class="article">

    <h1> Data flow analysis in Datalog</h1>
    <p>
      Suppose we are given the following code (line numbering omitted):
    </p>
    <div style="background: rgb(230, 230, 230); width: fit-content; padding-right: 35px; margin-bottom: 40px;">
      <pre style="border: 0ch; width: fit-content;">
     <code  class="language-pascal">
      x = source()
      y = x + z
      if (y > 0){
        y = x + 4
      }
      else {
        z = 2
      }
      sink(y)
     </code>
     </pre>
  </div>
    
     
  

  <p>
    The semantics are
    <ul>
      <li> Reading untrusted input<code> x = source()</code></li>
      <li>
        Assignments: <code> y = x + z </code> 
      </li>
      <li>
        Branching
      </li>
      <li>
        Output sink: <code>sink(y)</code> outputs the value of variable <code>y</code>  to output 
      </li>
    </ul>
  </p>

  <p>
    To write a datalog program that captures these semantics (encodes these programs), then we have:
  </p>

  <p>
    
    

    <pre style="border-style: solid; border-radius: 0.5cm; border-color: blueviolet; overflow: scroll;">

  // <code>L: X = source()</code> 
  <code>.decl source(L: number, X: symbol)</code>  

  // <code>L: sink(X)</code> 
  <code>.decl source(L: number, X: symbol)</code> 

  // <code>L: X = Y+Z</code> 
  <code>.decl assign(L: number, X: symbol, Y: symbol, Z: symbol)</code> 

  // <code>L1: ...</code> 
  // <code>L2: ...</code> 
  <code>.decl next(L1: number, L2: number)</code> 

  // <code>L1: if(X) { </code> 
  // <code>L2:  ... </code> 
  // <code> } </code> 
  // <code>L3:  ... </code> 
  // <code> } else { </code> 
  <code>.decl if(L1: number, L2: number, L3: number)</code> 

  // <code>if   { </code> 
  // <code>    ... </code> 
  // <code> L2: } </code> 
  // <code>else { </code> 
  // <code>     ... </code> 
  // <code> L3: } </code> 
  // <code> L1: ... </code> 
  <code>.decl join(L1: number, L2: number, L3: number)</code> 
  </pre>
  
  </p>

  <p>
    A value is unsanitized if its value is derived from user input (ignoring implicit control-flow dependencies). 
    Here we see a datalog program that calculates at every line all tainted values <b> before and after </b>
execution of that line and calculates if a sink may accept an unsanitised user input:
  </p>

<p>
  
  <pre style="border-style: solid; border-radius: 0.5cm; border-color: blueviolet;overflow: scroll;">

  // derived predicates
  .decl taintedBefore(L:number, X:symbol)
  .decl taintedBefore(L:number, X:symbol)

  // The sink argument at label L is tainted
  .decl unsanitized(L: number)
  .output unsanitized

  // After execution of source at line L (unsanitised user input) 
  // the value X after line L is unsanitised
  taintedAfter(L, X) :- source(L, X).

  // Line L is unsanitized if X is the outputted at line L 
  // and X was tainted before line L
  unsanitized(L) :- sink(L, X), taintedBefore(L, X).

  // X is tainted after line L if it was tainted 
  // before line L and it was not assigned something else
  taintedAfter(L, X) :- taintedBefore(L, X), !assign(L, X, _, _).
  
  // X is tainted after L if it was assigned at a tainted value on L
  taintedAfter(L, X) :- assign(L, X, Y, _), taintedBefore(L, Y).
  taintedAfter(L, X) :- assign(L, X, _, Z), taintedBefore(L, Z).

  // X is tainted before a line if it was tainted 
  // after an earlier line (all possible cases)
  taintedBefore(L2, X) :- next(L1, L2), taintedAfter(L1, X).
  taintedBefore(L2, X) :- if(L1, L2, _), taintedAfter(L1, X).
  taintedBefore(L3, X) :- if(L1, _, L3), taintedAfter(L1, X).
  taintedBefore(L1, X) :- join(L1, L2, _), taintedAfter(L2, X).
  taintedBefore(L1, X) :- join(L1, _, L3), taintedAfter(L3, X).
  </pre>

</p>
  <p>
    Now assume we have a sanitize function <code>sanitize(x)</code> which can be used to "sanitize" variables. To include this functionality in Datalog, note that
    we already had a similar functionality when we specified x to be tainted-after only if it is NOT assigned to something else. 
  </p>

  <pre style="border-radius: 0.2cm;">

  taintedAfter(L, X) :- taintedBefore(L, X), !assign(L, X, _, _), !sanitize(X).
  </pre>
  <p>
    The full project for a taint-analyzer on Ethereum smart contracts can be found at this 
    <a href="https://github.com/AmroAbdrabo/ethereum-sc-taint-analyzer"> GitHub link</a>. As for now, 
    let us focus on ways to attack models so that they (perhaps correctly) output "garbage dump" when they are fed an image of a WOKO student housing complex.
  
  </p>
  
     

  <h1> Black-box Attacks on Machine Learning Models</h1>
  
 <p>
  The defining question of this topic is whether ML can be deployed to the real world. Let us see some of the attacks against ML models.
 </p>
 <p>
  Imagine you are training a model to detect pneumothorax in lungs using a chest X-ray. The doctor attaches a drain to the chest and the patient 
  has an X-ray of his chest taken. Now, when the ML model is trained on this image, it will notice the presence of a drain. Of course, the probability that a 
  patient has pneumothorax given he has a drain is much higer than the probability of them having the disease with no drain attached. And that is exactly what the model would do.
  Given an X-ray showing a drain, its prediction will be biased towards showing presence of the disease, unlike the doctor who would make his decision independent of the drain (to some extent).  
 </p>
<p>
  There are certain threats to ML security:
</p>
<p>
  <ul>
    <li>Model inversion: inverting the model to obtain the dataset</li>
    <li>
      Membership inference: given the model, knowing whether a sample was used in the training dataset
    </li>
    <li>
      Adversarial attack: given a correctly classified image, can the image be slightly changed so that it is no longer correctly classified
    </li>
    <li>
      Data poisoning: adversary injects into the training set a few correctly classified images with a small and specific pixel pattern that biases the model towards that pixel pattern
    </li>
    <li>
      Backdoor ML: involves the use of data poisoning but with the attack component being incorrect label produced during inference
    </li>
  </ul>
</p>
<p>
  All images are affected by adversarial attacks, and even the best model is affected by adversarial attacks. 
</p>
<p>
  The problem is that state of the art models are accurate but very brittle.
</p>

<h2>
  Adversarial robustness
</h2>
<p>
  In order to achieve adversarial robustness, one must specify two components: 
  <ul>
    <li>
      Specification: what are the allowed changes to the image (e.g. rotation, sheering, lighting, scaling, masked parts, etc.)  
    </li>
    <li>
      Optimization: given the image, embedded into the input space, and the allowed modifications (defining a hyperplane of samples around the original input), can we obtain an incorrect label for a sample from this hyperplane. 
    </li>
  </ul>

  <img src="img/spec_opt.PNG" alt="" style = "width: 90%; height: auto; margin-left: auto; margin-right: auto; display: block;">  
</p>

<p>
  Recall the task of model training is to find \[ \text{argmin}_{\theta} E_{(\textbf{x, y}) \sim  D} \ loss(\theta, \textbf{x}, \textbf{y})    \]
</p>
<p>
  However, for adversarial attack the task is to find,  \[  \text{argmax}_{|| \delta|| < \epsilon }  \ loss(\theta, \textbf{x}+\delta, \textbf{y})  \]
</p>
<p>
  with the update being,
</p>
<p>
  \[\delta \leftarrow \delta - \nabla loss(\theta, \textbf{x}+\delta, \textbf{y}) \]
</p>
<h2>
  Attack taxonomy
</h2>
<p>
  <img src="img/att_tax.png" alt="" style = "width: 90%; height: auto; margin-left: auto; margin-right: auto; display: block;">  
</p>
<p>
  Note that as we go right-down the attack difficulty increases, and that we can aim for attacks that change the label to some defined target label or just an incorrect label (easier).
</p>
<h2>
  Generating Perturbations
</h2>
<br>


<p class = "subtitle">Algorithm I: Random Perturbation</p>
<div class="algo">
  <pre class="algo_pre">
 <code  class="language-python">
  Œ¥ = 0
  while y != argmax<sub>i</sub> f<sub>i</sub>(x+Œ¥):
    Œ¥ <- U(0,1)  
 </code>
 </pre>
</div>
<p>
  The algorithm keeps sampling a uniformly distributed perturbation until the classifier outputs the adversarial target class. 
  This is of course the simplest approach and achieves 0 to 10% attack success rate (ASR) even after half a million queries. 
</p>
<p class = "subtitle">Algorithm II: Genetic Perturbations</p>
<p>
  The high level idea is creating initial generation, with size N, based on the original input <b>x</b> and the p-code explains the rest
</p>
<div class="algo" style="width: fit-content">
  <pre class="algo_pre">
 <code  class="language-python">
  // Create initial generation
  For i = 1..N 
    P <sup>0</sup><sub>i</sub> <- <b>x + U(-Œ¥, Œ¥)</b> 
  For all generations do:
      For every member of the generation do:
          Compute the fitness of the member 
      If label of "most fit" member is the target t:
          return success
      Compute selection probabilities ("fitter" members more likely selected)
      For every member of the generation do:
          Apply mutations and clipping to new child input 
          Add mutated child to next generation
      Adaptively update Œ± (mutation range) and œÅ (mutation prob.) variables 
 </code>
 </pre>
</div>
<p>
  The algorithm achieves 95% to 100% ASR using around 100,000 queries.  A possible optimization is that instead of independently modifying 
  every single pixel, to instead modify nearby pixels together using a smaller noise mask:
</p>
<img src="img/genatt.PNG" alt="" style = "width: 70%; height: auto; margin-left: auto; margin-right: auto; display: block;"> 
<p>
  This in turn drives the number of queries down to ~20,000.
</p>
<p class = "subtitle">Algorithm III: SimBA (untargeted)</p>
<div class="algo">
  <pre class="algo_pre">
 <code  class="language-python">
  Œ¥ = 0
  p<sub>y</sub> = f<sub>y</sub>(x)
  while y = argmax<sub>i</sub> f<sub>i</sub> (x+Œ¥):
    pick randomly and without replacement q ‚àà Q
    for ùõº ‚àà {ùúÄ, -ùúÄ} do
      p<sub>y</sub><sup>new</sup>  = f<sub>y</sub> (x + ùõø + ùõºq) 
      if p<sub>y</sub><sup>new</sup> < p<sub>y</sub>
          ùõø = ùõø + ùõºq
          p<sub>y</sub> = p<sub>y</sub><sup>new</sup>
          break
 </code>
 </pre>
</div>

<p>
  where Q is an orthonormal basis for image inputs. This basis can be the normal cartesian basis (if you think of each N-by-N image as vector of length \(N^2\) then this is the normal canonical basis for these vectors).
  Another basis is the DCT (discrete cosine transform) basis (non-coincidentally used in JPEG image compression):
</p>

<p>
  The idea is that 9x9 images are encoded using 9 numbers which represent the coefficients of the following 9x9 single-channeled matrices:
  <img src="img/dct.PNG" alt="" style = "width: 30%; height: auto; margin-left: auto; margin-right: auto; display: block;"> 

</p>
<p>
  Each of these 9 squares has entries determined by the discrete cosine function sampled at fixed points. Reducing the size of the basis to something below 9 will result in some compression for 9x9 images (see weitz.de/dct/).
  Same idea applies for 32x32 images using 32*32=1024 DCT bases matrices. Hence, dimensionality reduction can be achieved by limited the number of basis matrices.  

</p>
<p>
  For untargeted adversarial attacks, the DCT-basis SimBA algorithm performs better with ~1000 queries (~97% ASR). However, the Cartesian-basis SimBA performs better on the 
  targeted case with ~10,000 queries and 100% ASR.  
</p>
<p class = "subtitle">Algorithm IV: Boundary Attack</p>
<p>
  The most beautiful attack in my opinion: this attack walks in reverse by starting with a random noise image, represented as <b>x</b> in original input space, and then repeatedly does a step which consists of a random orthogonal step followed by a step toward the original meaningful image. 
  The algorithm can be visualized as:
  <br>
  <img src="img/attboundary.PNG" alt="" style = "width: 80%; height: auto; margin-left: auto; margin-right: auto; display: block;"> 

</p>
<br>  
<p>
  The idea is summarized as,
  <blockquote style="font-size: x-large">
    How close can we get to the actual image so as to resemble the image while still being classified incorrectly?
  </blockquote>
</p>
<p>
  This attack is decision based as it relies only on hard labels and uses gaussian noise for the orthogonal perturbations.
</p>
<p class = "subtitle">Algorithm V: Guessing smart</p>
<p>
  Also decision-based attack, this attack takes two images, the original and the target label, and applies boundary attack by changing only 
  the relevant regions in the original image. This relevant region is obtained by the mask which emphasizes the areas of both images which are different. 
  This helps prevent modiying background pixels depicting the sky for example. 
</p>

  </main>
</div>
</body>
</html>
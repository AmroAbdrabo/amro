<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href = "syssec.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title>Machine Perception</title>
</head>
<body>


    <div class="topnav">
        <a class="active" href="index.html">Home</a>
      </div>
      
     
   
<script async="async" defer="defer" src="https://buttons.github.io/buttons.js"></script>
<div class="container" style = "grid-template-rows: 310px auto 150px;">
  <div class="meta"> 
   
    <div class="info" style="width: 100vw;"> 
      <h1>Machine Perception </h1>
      <p class="subtitle">Homework and Lecture Notes</p>
      <div class="author" >
        <div class="authorImage"></div>
        <div class="authorInfo">
          <div class="authorName"><a href="https://github.com/AmroAbdrabo">Amro Abdrabo</a></div>
          <div class="authorSub">Mar. 4 <span class="median-divider"> </span> </div>
        </div>
      </div>
    </div>
  </div>
  <main class="article">

    <h1 style="font-size: 2.2em; font-family: Calibri;">Lecture 1: Training Neural Networks </h1>
    <p class="subtitle">presented by Prof. Dr. Siyu Tang</p>
    <p>
        <span class="first-letter">T</span>he simplest perceptron can be represented as \[y =  \sigma(w^Tx+b) \]
        where \(w = [w_0, w_1, ..., w_n]\) is a set of weights, and \(x = [x_0, x_1, ..., x_n]\) is the feature vector, and the bias is a vector \(\mathbf{b}\).
        We need to prove that activation functions can only approximate linear functions: to show we use induction. The base case is easy \[ \mathbf{x^{(1)}} = W^{(1)} \mathbf{x}+b^{(1)} \Rightarrow \mathbf{z^{(1)}}  = \sigma(\mathbf{x}^{(1)}) = A^{(1)}\mathbf{x}+B^{(1)}  \]
        By induction, assume that \(\mathbf{z}^{(l)} = A^{(l)}\mathbf{x}+ B^{(l)} \) we prove that \(\mathbf{z}^{(l+1)} = A^{(l+1)}\mathbf{x}+ B^{(l+1)}\). Note that \(\mathbf{z}^{l+1} = \sigma(\mathbf{x}^{l+1}) = \alpha \mathbf{x}^{(l+1)}+\beta I  \) by linearity of \(\sigma\). 
        Now \(\mathbf{x}^{(l+1)} = W^{(l+1)} \mathbf{z}^{l} + \mathbf{b}^{(l+1)} = W^{(l+1)}(A^{(l)}\mathbf{x}+ B^{(l)})+ \mathbf{b}^{(l+1)} \) which means that \[\mathbf{z}^{(l+1)} = \alpha (W^{(l+1)}(A^{(l)}\mathbf{x}+ B^{(l)})+ \mathbf{b}^{(l+1)})+\beta I\]
        by simplifying further we prove the claim. 
     
      
    </p>

        <p>
            Now we want to model the XOR function linearly, then we use the normal equation \( \beta = (X^T X)^{-1} \cdot (X^Ty) \) note that we let the first element of the feature vector be 1 to 
            represent the bias. Using this equation we get that least squares solution is \(\mathbf{w} = 0\) and \(b=\frac{1}{2}\). Since XOR is not linearly separable we need to map \((x_1, x_2)\) to \((h_1, h_2)\) in the hyperspace. It turns out 
            we can learn the XOR using a multi-layered perceptron using the function \( f(\mathbf{x}; W, \mathbf{c}, \mathbf{w}, b) = \mathbf{w}^T\max \{ 0, XW+\mathbf{c}\} + b \) with \(b=0, \mathbf{w} = [1, -2]^T, \mathbf{c} = [0, -1]^T, \text{ and } W =\begin{bmatrix}
            1 & 1\\
            1 & 1
            \end{bmatrix} \).
        </p>
        <p>
            <b>Universal Approximation Theory</b>
            <br>
            Given that \(\sigma \in C^{\infty}(\mathbb{R}) \) is non-linear (e.g. sigmoid) what type of functions can we learn. We can learn continuous functions. Given enough hidden units, one layer is enough in theory. In practice, deeper net is better. Using a MLP, we have 
            \[
            \exists g(x) \text{ as NN}, g(x) \approx f(x), \text{ and } |g(x) - f(x)| < \epsilon 
            \]
            To prove the universal approximation theory, one shows that a two-layer NN can model a bump, and that the more hidden units there are the more bumps we achieve. Of course, these bumps represent the localized approximations to the function \(f(x)\). 
        </p>

        <p>
            <strong>Homework 1</strong>
            <br> <br>
            <b>Multivariable Chain Rule:</b> <br>
            The gradient \(\nabla_x f\) of a function \(f: \mathbb{R}^n \rightarrow \mathbb{R} \), \(f: \mathbf{x} \mapsto y \) is defined as \(\nabla_x f = [ \frac{\partial y}{\partial x_1}, ..., \frac{\partial y}{\partial x_n} ]^T \in \mathbb{R}^n\). The Jacobian 
            \(J(g)_x\) of a function \(g: \mathbb{R}^n \rightarrow \mathbb{R}^m \), \(g: \mathbf{x} \mapsto \mathbf{y} \) is defined as a matrix of size \(\mathbb{R}^{m\times n}\), where each element \(J(g)_{\mathbf{x}, i, j}\) is defined as \( J(g)_{\mathbf{x}, i, j} = \frac{\partial y_i}{\partial x_j} \). Therefore, the transposed gradient can be regarded as a special case of the Jacobian, where \(m=1 \). <br> <br>
            In higher dimensions, where \(\mathbf{g}: \mathbb{R}^n \rightarrow \mathbb{R}^m, \ \mathbf{x} \mapsto \mathbf{y} \) and \(f: \mathbb{R}^m \rightarrow \mathbb{R}^l, \ \mathbf{y} \mapsto \mathbf{z}  \) the chain rule states: \[
            \frac{\partial z_k}{\partial x_j} = \sum_{i=1}^{m} \frac{\partial z_k}{\partial y_i}\frac{\partial y_i}{\partial x_j}
            \] 
        </p>

        <p>
            <b>
                1.1 Chain rule for Jacobians
            </b>
            <br> Given the definitions above, show that \( \frac{\partial \mathbf{z}}{\partial \mathbf{x}} = \frac{\partial \mathbf{z}}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}} \). What are the dimensions? <br>
            <i>Hint:</i> first derive \(\frac{\partial z_k}{\partial \mathbf{x}}\). <br>
            <b>Solution:</b> Note first that the dimension of \(\frac{\partial \mathbf{z}}{\partial \mathbf{x}}  \) is \( l\times n\). The \(kth\) row is given by \(\frac{\partial z_k}{\partial \mathbf{x}}  \) and that the \(ith\) column in that row is given by 
            \(  \sum_{i=1}^{m} \frac{\partial z_k}{\partial y_i}\frac{\partial y_i}{\partial x_j}  \) simplifying the notation gives  \(  \sum_{i=1}^{m} \left(\frac{\partial z}{\partial y}\right)_{k, i}\left(\frac{\partial y}{\partial x}\right)_{i, j}  \) i.e. we are taking the dot product of 
            the \(kth\) row of \( \frac{\partial \mathbf{z}}{\partial  \mathbf{y}}\) with the \(jth\) column of \( \frac{\partial \mathbf{y}}{\partial  \mathbf{x}} \). 
        </p>
       
        <p>
            <b>
                Chain rule for graphs
            </b>
            <br>
            Given the neural net of architecture 
            <img src="img/mp12.png" alt="" style = "width: 60%; height: auto; margin-left: auto; margin-right: auto; display: block;">  
            derive \( \frac{\partial y}{\partial x_2}  \) <br>
            <b>Solution:</b> We follow the path to \(x_2\) from \(y\) but in this case we see that there are two paths. Thus, there will be two summands: \( \frac{\partial y}{\partial x_2} = \frac{\partial y}{\partial h_2} \frac{\partial h_2}{\partial c_1} \frac{\partial c_1}{\partial x_2} +  \frac{\partial y}{\partial h_2} \frac{\partial h_2}{\partial c_2} \frac{\partial c_2}{\partial x_2}   \) we thus get 
            \( \frac{1}{h_2}\sigma(c_1c_2 + h_1)(1 - \sigma(c_1c_2 +h_1))c_2 (1 - \tanh^2(\sum_{j=1} ... ))w_2  \) as the derivative of the branch that passes through \(c_1\). Note the derivative of \(\tanh\) is \(1-\tanh^2\), and the derivative of \(\sigma(u)\) is \(\sigma(u)(1- \sigma(u))\)
        </p>

        <h1 style="font-size: 2.2em; font-family: Calibri;">Lecture 4: Recurrent Neural Networks </h1>
        <p class="subtitle">Sequence modelling with neural nets. Presented by Sergey Prokudin</p>
        <h2>
          Sequence processing as a dynamical system
        </h2>
        <p style="margin-top: 1cm;">
          <img src="img/seqprocessing.png" alt="" style = "width: 90%; height: auto; margin-left: auto; margin-right: auto; display: block;">  
          where \(  \mathbf{x^0}, ...\mathbf{x^{t}} \) is the input sequence and \(\mathbf{s^0}, ..., \mathbf{s^t}\) is the sequence of hidden states (memory), and \(\mathbf{y^0},...,\mathbf{y^t}\) is the sequence of model outputs, and finally
          \(f_{W}(\mathbf{x^t}, \mathbf{s^t})\) our NN with parameters <i>W</i>.  Note \(f_W: \mathbf{x^t}\times \mathbf{s^t} \rightarrow \mathbf{y^t}\times \mathbf{s^{t+1}}\)
        </p>
        <p>
          The goal is to find \(W^* = \text{arg min}_{W} \sum_{t=1}^T \mathcal{L}(y_{gt}^t, y^t) \)
        </p>
        
        <h2>Modelling \(f_{W}\): Vanilla RNN</h2>
        <p>
          \(f_{W}: x^t \times s^t \rightarrow y^t \times s^{t+1} \), and weights \(\{W_s, W_x, W_y\}\) where biases are omitted for clarity and we have the basic RNN
          \( s^{t+1} = \tanh (W_s s^{t}  + W_x x^t ) \) and \( y^t  =W_y s^t \)
        </p>

        <h2>Backpropagation</h2>
        <p>Dramatically simplified scenario: generating a sequential output from a single initial input, no internal state and no activation functions </p>
        <p>
          <img src="img/simpexample.png" alt="" style = "width: 90%; height: auto; margin-left: auto; margin-right: auto; display: block;">  
          \(\mathbf{y}^0 = W\mathbf{x^0}; \mathbf{y^1} = W\mathbf{y^0}=WW\mathbf{x^0},..., \mathbf{y^2} = W\mathbf{y^1};... \mathbf{y^t} =W\mathbf{y^{t-1}} = W^{t+1}\mathbf{x^0}  \)
          <br>

          and \( \mathcal{L}(\mathbf{y_gt^t}, \mathbf{y^t}) = || \mathbf{y_gt^t} - \mathbf{y^t} ||_2^2 \)
        </p>
        <p>
          For simpicity assume that \(\mathbf{y^t} \in \mathbb{R}, W \in \mathbb{R}, \mathbf{x^0} \in \mathbb{R}\) then 
          <img src="img/rnnback.png" alt="" style = "width: 90%; height: auto; margin-left: auto; margin-right: auto; display: block;">  
          so we see we have a problem of vanishing and exploding gradients based on \(|W| < 1\). Same idea applies to higher dimensional \(W\) via eigenvalues. 
        </p>
        <h2>Solution: Long Short Term Memory </h2>
        <p>
          <b>Main idea</b>: have more control over hidden state updates by learning what to keep and what to forget. Compared to vanilla RNNs, this means slightly more complicated internal state update, resulting in better training dynamics. 
        </p>

        <h2 style="color: cyan; margin-top: 0cm;"> LSTM - S recursion</h2>
        <p style="border: 2px solid cyan; font-size: larger; width: 90%; height: auto; margin-left: auto; margin-right: auto; display: block; text-align: center;">
          \( \mathbf{s}^{t} = \mathbf{f}^{t} \circ \mathbf{s}^{t-1} + \mathbf{u}^t \circ \mathbf{s}_{new}^{t}  \), 
        </p>
        <p>
           \(\circ\) denotes element-wise multiplication
        </p>

        <p>
          <ul>
            <li>
              Forget gate: \(\mathbf{f}^t = \sigma( W_{fs} \mathbf{s}^{t-1} + W_{fx} \mathbf{x}^{t-1} )\)  elementwise between 0 and 1 - how much of the previous state information do we want to forget <br>

            </li>
            <li>
              Update gate: \(\mathbf{u}^t = \sigma(W_{us} \mathbf{s}^{t-1} + W_{ux}\mathbf{x}^{t-1})\) elementwise between 0 and 1 - how much of the new state to keep 
            </li>
            <li>
              State update: \(\mathbf{s}_{new}^t = \tanh ( W_{ss}\mathbf{s}^{t-1} + W_{sx}\mathbf{x}^{t-1})\)
            </li>
          </ul>
          
        </p>

        <h2 style="color: cyan; margin-top: 0cm;"> LSTM - Y</h2>
        <p style="border: 2px solid cyan; font-size: larger; width: 90%; height: auto; margin-left: auto; margin-right: auto; display: block; text-align: center;">
          \( \mathbf{y}^{t} = \sigma(W_{ys} \mathbf{s}^{t} + W_{yx} \mathbf{x}^t )\circ \tanh \mathbf{s}^t \)
        </p>
        
        <h1 style="font-size: 2.2em; font-family: Calibri;">Lecture 5: Deep Generative Models </h1>
        <p class="subtitle">presented by Prof. Dr. Otmar Hilliges</p>

        <p>
          <span class="first-letter">
            
          </span>
        </p>






</main>

</div>
    
</body>
</html>